{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Machine Learning (ML) - part II\n",
    "\n",
    "by [@barbara_plank](https://twitter.com/barbara_plank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine learning = learning from data\n",
    "\n",
    "learning what? \n",
    "\n",
    "to make **predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The goal of machine learning is to find a function $f$ that, given some input $x$, produces predictions for that input, $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To visualize the whole:\n",
    "\n",
    "<img src=\"pics/learning.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview - What we need\n",
    "\n",
    "1. Data\n",
    "  * what your data looks, the input $X$ and output (labels) $Y$ \n",
    "2. Features\n",
    "  * how to represent your data (the actual features): how to decompose $X$ into its parts by $\\phi$\n",
    "3. Model/Algorithm\n",
    "  * the machine learning algorithm used \n",
    "4. Evaluation\n",
    "  * how to measure how good your model is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification vs Regression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In **supervised machine learning** the y’s are given, and are called the labels. They can be categorial, like ”sports”, ”news”, etc. or numerical, e.g. 7, 8,10. If the labels are categorical we speak of classification, in case of numerical labels the task is regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example - first step with scikit-learn :Training a classifier on the IRIS dataset\n",
    "\n",
    "In this section, we will train a classifier on the [IRIS data set](http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html). \n",
    "\n",
    "<img src=\"pics/iris.png\" width=300>\n",
    "This data set about plants is included in sklearn and already ready to use, meaning that features are already extracted for the data instances x, and each training instance has an associated class label y. In the next section we will see how to extract features and use them in sklearn.\n",
    "The iris data set consists of 150 training instances with 3 classes (setosa,versicolor,virginica). Technically, it is stored as a python dict, thus we can see dict.keys() to inspect what it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The iris data set consists of 150 training instances with 3 classes (setosa,versicolor,virginica). Technically, the whole information is stored as a python dictionary, we can see `dict.keys()` to inspect what it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], \n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the possible Y's (labels/categories)\n",
    "iris['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Looking at the actual label valuess of the IRIS dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "## in this example the labels are actually encoded as numbers! \n",
    "# NB. sklearn can also directly use labels (as we will see later), but many other ML packages expect labels as numbers\n",
    "print(np.unique(iris_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do the examples/data instances/cases look like?\n",
    "\n",
    "Each training instance consists of 4 attributes (thus is 4-dimensional, or a vector with 4 dimensions), in this case numerical measurements. We can get a description using feature_names. For instance, lets look at the first data instance $x_0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.1,  3.5,  1.4,  0.2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"http://www.wpclipart.com/plants/diagrams/plant_parts/petal_sepal_label.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The class labels y for the instances are actually stored in iris[’target’] as integers (indices corresponding to the respective target_names entry). Thus, the first instances is of type setosa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['target'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we are ready to train a classifier on 80% of the data, and evaluate on the remaining 20%. We will train both a k-nearest neighbor classifier as well as logistic regression model, and evaluate both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#inst train: 120\n",
      "#inst test: 30\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Pred: [2 1 0 0 2 0 2 2 0 2 1 1 2 2 1 0 1 2 2 1 1 0 0 0 2 0 0 2 0 1]\n",
      "Gold: [2 1 0 0 2 0 1 2 0 2 1 1 2 2 1 0 1 2 2 1 1 0 0 0 2 0 0 2 0 1]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     setosa       1.00      1.00      1.00        11\n",
      " versicolor       1.00      0.89      0.94         9\n",
      "  virginica       0.91      1.00      0.95        10\n",
      "\n",
      "avg / total       0.97      0.97      0.97        30\n",
      "\n",
      "[[11  0  0]\n",
      " [ 0  8  1]\n",
      " [ 0  0 10]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 3 classes, 150 instances: X=iris[’data’]  y=iris[’target’]\n",
    "iris = datasets.load_iris()\n",
    "# create random permutation with seed (uncomment to get fixed set)\n",
    "#np.random.seed(1253)\n",
    "indices = np.random.permutation(len(iris['data']))\n",
    "# split in 80% train, 20% test - NB: in a real setup we would split off a separate dev/development set and keep test for the end\n",
    "len_test = int(len(iris['data'])*0.2)\n",
    "# train part (all except test part)\n",
    "X_train = iris['data'][indices[:-len_test]]\n",
    "y_train = iris['target'][indices[:-len_test]]\n",
    "\n",
    "# test part\n",
    "X_test = iris['data'][indices[-len_test:]]\n",
    "y_test = iris['target'][indices[-len_test:]]\n",
    "\n",
    "# output statistics\n",
    "print(\"#inst train: %s\" % (len(X_train)))\n",
    "print(\"#inst test: %s\" % (len(X_test)))\n",
    "# learn classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf)\n",
    "y_pred= clf.predict(X_test)\n",
    "print(\"Pred:\", y_pred)\n",
    "print(\"Gold:\", y_test)\n",
    "# get accuracy\n",
    "print(classification_report(y_test, y_pred, target_names=iris['target_names']))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wFaL/BDkAOiCgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f490ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "conf = confusion_matrix(y_test, y_pred)\n",
    "plt.imshow(conf, cmap='binary', interpolation='None')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted   0  1   2  All\n",
      "True                     \n",
      "0          11  0   0   11\n",
      "1           0  8   0    8\n",
      "2           0  1  10   11\n",
      "All        11  9  10   30\n"
     ]
    }
   ],
   "source": [
    "## an alternative\n",
    "import pandas as pd\n",
    "def crosstab(pred, gold):\n",
    "    y_true = pd.Series(gold)\n",
    "    y_pred = pd.Series(pred)\n",
    "    print(pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "    \n",
    "crosstab(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Label encoding\n",
    "\n",
    "Notice that in the example above the labels were encoded as numbers. However, we could also use the names directly. \n",
    "\n",
    "There is a class in sklearn that maps between labels and numbers, the `LabelEncoder`. Have a look at it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica'], \n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_y = np.array([iris['target_names'][index] for index in iris['target']])\n",
    "data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Exercises:\n",
    "\n",
    "* Modify the example above so that it used the named labels directly.\n",
    "* Use the K-nearest neighbor classifier for the Iris dataset. Compare its performance to logistic regression.\n",
    "* (Advanced): Use the `DictVectorizer` to represent the input data, i.e., convert each features to a dictionary key-value pair and use the `DictVectorizer` to convert it then to the input the classifier uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Features, and feature templates\n",
    "\n",
    "In NLP, we typically have a *lot* of features (not just the 4 we saw in the IRIS example). Typically, we work with entire *vocabularies*. When we speak of a *feature* in NLP we typically mean a *feature template* (which gets instantiated to a set of values). For example, when we say we use unigrams (single words) as feature, this is actually a feature template that gets instantiated. Or when we say we use POS tags. Internally, these every value of a feature template is mapped to a unique feature (feature id), and if this feature is 'on' (or active), we'll see a 1 in the place of the vector that represents this features (e.g. POS tag is \"DET\").\n",
    "\n",
    "You can imagine that every instance is represented as a long vector (a high-dimensional vector; each dimension is a features). If you have 10000 features, you can imagine a vector of 10000 length, or 1000 dimensions. For every feature that is active for a given example, you turn the vector dimension 'on' (save it as a 1). \n",
    "\n",
    "Now in practice, we won't work with superlong vectors. They are becoming easily too big to process. Why?\n",
    "\n",
    "Read 'bag of words' of the tutorial: [http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) \n",
    "\n",
    "* features, feature templates\n",
    "* sparse representations\n",
    "\n",
    "Sklearn is handling the data in sparse format for you. You don't need to worry about it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Sentiment Classification\n",
    "\n",
    "Go to the folder 'exercise'. Retrieve the code. Go through the code with your neighbor. Fill\n",
    "out the blanks. Make sure you understand all parts of the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Machine Learning\n",
    "\n",
    "For you there to read, a more formal view.\n",
    "\n",
    "### Input and output\n",
    " \n",
    "The goal of supervised machine learning is to find a function $h$ that maps from some percept or input $x$ to a label $y$. What $x$ and $y$ are depends on the task. Many banks, for instance, use a learned function to decide whether to give credit to a customer or not. Here, $x$ is the credit application and $y$ is the outcome: approved or declined. In NLP, $x$ could be a tweet and $y$ could be its sentiment, or $x$ could be a sentence and $y$ is syntactic parse tree; and so forth. Let $x \\in \\mathcal{X}$ (input space) and $y \\in \\mathcal{Y}$ (label space).\n",
    "\n",
    "NLP applications almost always have **discrete** output spaces. In these lectures $y$ will either be an integer (for classification) or a vector of integers (for structured prediction). \n",
    "\n",
    "### Target and hypothesis function\n",
    "\n",
    "We’ll make the assumption that there exists an **unknown target function** which is solving the problem we’re interested in:\n",
    "\n",
    "$$f: \\mathcal{X} \\mapsto \\mathcal{Y}$$\n",
    "\n",
    "This, of course, is a bit of a fiction. It doesn’t really exist anywhere, but it’s a useful fiction because it allows us to describe the goal, which is to learn a **hypothesis function** $h$ that is as close as possible to the target function. Naturally, the hypothesis function performs the same mapping as the unknown target function:\n",
    "\n",
    "$$h: \\mathcal{X} \\mapsto \\mathcal{Y}$$\n",
    "\n",
    "### Dataset \n",
    "\n",
    "It gets worse before it gets better. Not only is our target function unknown, we also don’t know the true distribution of our inputs $P(x)$. We don’t know which tweets will be written or the kinds of backgrounds people who apply for credit will have.   \n",
    "\n",
    "Supervised learning rests on the idea that we can get a limited number of examples (i.e. **a sample**) \n",
    "\n",
    "$$x_1, \\ldots, x_n \\sim P(x)$$\n",
    "\n",
    "from the unknown input distribution $P(x)$, and that we (somehow) can evaluate on the unknown target function $f$ on these examples. \n",
    "\n",
    "Putting this together yields the concept of a **training set**:\n",
    "\n",
    "$$\\mathcal{D}_t = \\{(x_1, f(x_1) ), \\ldots (x_n, f(x_n)) \\}$$\n",
    "\n",
    "How do we gain access to the unknown target function? The bank might look at past credit applications together with the decisions. In NLP we often ask *people* to annotate.\n",
    "\n",
    "#### Unsupervised and semi-supervised learning\n",
    "\n",
    "It’s easy to imagine a situation where we could arrange to get a large sample of data from $P(x)$ without labels being included in the deal. The setting in which there are no labels at all is called **unsupervised learning**. When unlabeled data is available in addition to a labeled dataset this is **semi-supervised learning**. \n",
    "\n",
    "### Feature representation\n",
    "\n",
    "We’ll never have to read the same Twitter message twice, hopefully. By the time a failed credit application is resubmitted, the customer’s circumstances are likely different, and so the  application isn’t the same anymore. “You cannot submit a credit application twice,” as Heraclitus might have said. \n",
    "\n",
    "This poses a problem in that we wish to learn from the past, but whatever happened in the past it will not happen *exactly* like that again. Instead something *similar* might happen. So we need a way to break up our observations (the $x$es) to make them comparable even if the don’t match exactly. \n",
    "\n",
    "Luckily, our observations are typically not unique snowflakes, but can decomposed into **features** in some **feature space** $\\mathcal{F}$. Even though the learner might not have seen the new example exactly, it might have seen similar examples (or parts of the current example), and thus still be able to make a prediction.\n",
    "\n",
    "Specifically, each input example is transformed into a suitable **input representation** for the learning algorithm by a **feature function** $\\phi(x)$. The feature function $\\phi(\\cdot)$ maps examples from the input space to the feature space:\n",
    "\n",
    "$$\\phi: \\mathcal{X} \\rightarrow \\mathcal{F}$$\n",
    "\n",
    "Typically, the $\\phi(x)$ is a real-valued vector of some fixed dimension $d$, i.e. \n",
    "\n",
    "$$\\mathcal{F} = \\mathbb{R}^d$$\n",
    "\n",
    "Note that the $\\phi$ feature function is deterministic and not a part of the learner. Traditionally, a large body of work in NLP focused on finding better ways to map from input to feature representations for specific tasks by hand. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "* [sklearn: Working with text data](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
