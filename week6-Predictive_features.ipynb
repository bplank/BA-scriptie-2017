{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Finding predictive features\n",
    "\n",
    "In Machine Learning, it is important to find good features. Such features will help to build better classifiers. Once we have trained a good model, we might also want to know which features were the particularly helpful. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier on the IRIS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will again train a classifier on the [IRIS data set](http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html) but now we will investiate which features were the most predictive.\n",
    "\n",
    "<img src=\"pics/iris.png\">\n",
    "Recall: this data set about IRIS flowers is included in sklearn and already ready to use, meaning that features are already extracted for the data instances x, and each training instance has an associated class label y. \n",
    "The iris data set consists of 150 training instances with 3 classes (setosa,versicolor,virginica). Lets train a classifier and evaluate it.\n",
    "\n",
    "And a reminder:\n",
    "<img src=\"http://www.wpclipart.com/plants/diagrams/plant_parts/petal_sepal_label.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# 3 classes, 150 instances: X=iris[’data’]  y=iris[’target’]\n",
    "iris = datasets.load_iris()\n",
    "indices = np.random.permutation(len(iris['data']))\n",
    "# split in 80% train, 20% test\n",
    "len_test = int(len(iris['data'])*0.2)\n",
    "# train part (all except test part)\n",
    "X_train = iris['data'][indices[:-len_test]]\n",
    "y_train = iris['target'][indices[:-len_test]]\n",
    "# test part\n",
    "X_test = iris['data'][indices[-len_test:]]\n",
    "y_test = iris['target'][indices[-len_test:]]\n",
    "# output statistics\n",
    "print(\"#inst train: %s\" % (len(X_train)))\n",
    "print(\"#inst test: %s\" % (len(X_test)))\n",
    "# learn knn classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf)\n",
    "y_pred= clf.predict(X_test)\n",
    "print(\"Pred:\", y_pred)\n",
    "print(\"Gold:\", y_test)\n",
    "# get performance scores\n",
    "print(classification_report(y_test, y_pred, target_names=iris['target_names']))\n",
    "print(\"accuracy: \",accuracy_score(y_test, y_pred))\n",
    "print(iris['target_names'])\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We now see that petal length is amongst the most predictive features. However, we do not see how predictive it was for each class. For this, we inspect the coefficient per class, as shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## Since this example has few features, we can look at them all:\n",
    "feature_names = iris['feature_names']\n",
    "all={}\n",
    "for class_num in range(0,len(clf.coef_)):\n",
    "     all[iris['target_names'][class_num]] = {\"Feature\":feature_names,\"Coefficients\":clf.coef_[class_num]}\n",
    "\n",
    "all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in real examples it is typical to only look at the top-n most predictive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=1\n",
    "feature_names = iris['feature_names']\n",
    "for class_num in range(0,len(clf.coef_)):\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[class_num], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    print(\"class_num\",class_num, iris['target_names'][class_num])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Extend the sentiment classification example from last week. Find the most predictive feature per class.\n",
    "\n",
    "Hint: you can use [the method below](https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=10):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i in range(0,len(clf.coef_)):\n",
    "        coefs_with_fns = sorted(zip(clf.coef_[i], feature_names))\n",
    "        top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "        print(\"i\",i)\n",
    "        for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "            print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "There is much more to be said about feature selection than what we cover here. Below are some pointers.\n",
    "\n",
    "* http://machinelearningmastery.com/an-introduction-to-feature-selection/\n",
    "* http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n",
    "* http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
