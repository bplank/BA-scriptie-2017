{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining heterogeneous feature spaces\n",
    "\n",
    "A times, we want to use different kinds of features in our classifier. In `sklearn`, there are two ways to combine features from different feature spaces:\n",
    "\n",
    "1. Using `FeatureUnion` to combine features from heterogeneous sources.\n",
    "2. Using `DictVectorizer` and define your own feature spaces.\n",
    "\n",
    "Each approach comes with its advantages and disadvantages, which we will discuss further below. Thus, balance the pro and cons of for yourself before deciding which approach you are gonna use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Using FeatureUnion\n",
    "\n",
    "Lets take our sentiment classification example, and combine two feature sources: word and character n-grams. To do so, we will use the class `FeatureUnion` which lets us easily combine build-in featurizers. Lets  go back to our sentiment analysis example, but add `FeatureUnion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train the classifier using word unigrams only, to see how the code changes when we include `FeatureUnion`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load data..\n",
      "split data..\n",
      "vectorize data..\n",
      "train model..\n",
      "predict..\n",
      "Accuracy: 0.767066766692\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# using a seed for replicability\n",
    "random.seed(113)\n",
    "\n",
    "def load_sentiment_sentences_and_labels():\n",
    "    \"\"\"\n",
    "    loads the movie review data\n",
    "    \"\"\"\n",
    "    positive_sentences = open(\"exercise/rt-polaritydata/rt-polarity.pos\").readlines()\n",
    "    negative_sentences = open(\"exercise/rt-polaritydata/rt-polarity.neg\").readlines()\n",
    "\n",
    "    positive_labels = [1 for sentence in positive_sentences]\n",
    "    negative_labels = [0 for sentence in negative_sentences]\n",
    "\n",
    "    sentences = np.concatenate([positive_sentences,negative_sentences], axis=0)\n",
    "    labels = np.concatenate([positive_labels,negative_labels],axis=0)\n",
    "\n",
    "    ## make sure we have a label for every data instance\n",
    "    assert(len(sentences)==len(labels))\n",
    "    data = list(zip(sentences,labels))\n",
    "    random.shuffle(data)\n",
    "    print(\"split data..\", file=sys.stderr)\n",
    "    split_point = int(0.75*len(data))\n",
    "    \n",
    "    sentences = [sentence for sentence, label in data]\n",
    "    labels = [label for sentence, label in data]\n",
    "    X_train, X_test = sentences[:split_point], sentences[split_point:]\n",
    "    y_train, y_test = labels[:split_point], labels[split_point:]\n",
    "\n",
    "    assert(len(X_train)==len(y_train))\n",
    "    assert(len(X_test)==len(y_test))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "## read input data\n",
    "print(\"load data..\", file=sys.stderr)\n",
    "X_train, y_train, X_test, y_test = load_sentiment_sentences_and_labels()\n",
    "\n",
    "print(\"vectorize data..\", file=sys.stderr)\n",
    "#vectorizer = CountVectorizer()\n",
    "#pipeline = Pipeline( [('vec', vectorizer),\n",
    "#                        ('clf', LogisticRegression())] )\n",
    "\n",
    "# use FeatureUnion instead\n",
    "pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('words', CountVectorizer()),\n",
    "        ])),\n",
    "        ('classifier', LogisticRegression())])\n",
    "\n",
    "print(\"train model..\", file=sys.stderr)\n",
    "pipeline.fit(X_train, y_train)\n",
    "##\n",
    "print(\"predict..\", file=sys.stderr)\n",
    "y_predicted = pipeline.predict(X_test)\n",
    "###\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have `FeatureUnion` in place, we can add further transformers to the list (a transformer needs to implement fit and transform function, see more info here: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html). Lets see if adding character n-grams helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train model..\n",
      "predict..\n",
      "Accuracy: 0.764066016504\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('words', CountVectorizer()),\n",
    "            ('chars', CountVectorizer(analyzer='char',ngram_range=(4,5), binary=True)),\n",
    "        ])),\n",
    "        ('classifier', LogisticRegression())])\n",
    "\n",
    "print(\"train model..\", file=sys.stderr)\n",
    "pipeline.fit(X_train, y_train)\n",
    "##\n",
    "print(\"predict..\", file=sys.stderr)\n",
    "y_predicted = pipeline.predict(X_test)\n",
    "###\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature ablation test\n",
    "\n",
    "Using `FeatureUnion` has a couple of pros and cons. It's **advantages** are:\n",
    "\n",
    "+ it's easy to combine different feature spaces; and\n",
    "+ it's quick to implement\n",
    "\n",
    "However, with `FeatureUnion` it is no longer straightforward to access the weight coefficient and thus inspect the features. As we will see later, it no longer supports the `show_most_informative_features` method [src: [method taken from here](http://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers)]\n",
    "\n",
    "However, there are still ways in order to gauge the effect of a certain feature group. In particular, we can do a **feature ablation test**. As the name already suggests, you leave out certain features and train a model without, and compare it to the model that includes the features. \n",
    "\n",
    "In particular, a feature ablation test is a set of experiments in which you remove a feature group at a time, and you observe how much your performance drops. This gives you an indication of how good the feature is for your prediction task. The more performance drops, the more useful the feature was for the prediction task. `FeatureUnion` is particularly helpful here as it allows quickly to 'turn off'/'turn on' certain feature groups and thus do feature ablation tests. \n",
    "\n",
    "Another advantage of `FeatureUnion` is that it is easy to use if you are working in a team, and team members contribute different features. However, if your aim is more at understanding what are important features, then it might be more fruitful to go with option 2, using the `DictVectorizer`.\n",
    "\n",
    "How can we add our own featurizer? Before we go into details of the `DictVectorizer`, lets have a look at a simple example (inspired from [1])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing your own vectorizer\n",
    "\n",
    "Suppose we want to add a feature that measures the length of a text (suppose for a minute that, say, tweet lenght is indicative for sentiment). To do so, we create our own featurizer, which is a subclass of `TransformerMixin`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our need class will need to implement both `fit` and `transform`, and it will extract features that are subsequently given to the `DictVectorizer`. \n",
    "\n",
    "Regarding `fit`, we can just leave the function emtpy (as no new vocabulary needs to be created). The action happens in `transform`: given the input data `X` (a list of lists of actual texts), we need to convert it to a representation where for every data instance we return a **dictionary** that holds our new feature and its feature value. Thus, in this simple case we represent each text by its length, so we add a feature 'length' that contains the length of the text as value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextStats(TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \" extract length of each data instance \"\n",
    "        out= [{'length': len(text)}\n",
    "                for text in X]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use this new feature and add it to our pipeline. Note: after calling `TextStats` we need to call `DictVectorizer`, which takes care of converting the text into the format `sklearn` internally uses (sparse feature representation). Let's import `DictVectorizer` (notice that is is no longer in the 'text' package, but is in the more general package!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train model..\n",
      "predict..\n",
      "Accuracy: 0.765566391598\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('words', CountVectorizer()),\n",
    "            ('stats', Pipeline([\n",
    "                ('selector', TextStats()),\n",
    "                ('statsFeats', DictVectorizer()),\n",
    "                            ])),\n",
    "        ])),\n",
    "        ('classifier', LogisticRegression())])\n",
    "\n",
    "print(\"train model..\", file=sys.stderr)\n",
    "pipeline.fit(X_train, y_train)\n",
    "##\n",
    "print(\"predict..\", file=sys.stderr)\n",
    "y_predicted = pipeline.predict(X_test)\n",
    "###\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the length feature doesn't help. This makes sense (would you really expect that the length of a text tell us much about sentiment?) However, this example code exemplifies you how you could use additional features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging with Spacy\n",
    "\n",
    "Assume we want to add POS tag information to our sentiment classifier. First, we need to tag our data.\n",
    "\n",
    "We here use a simple off-the-shelf tagger that is available for English, `spacy`. See: https://spacy.io/docs/usage/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now tag the sentiment example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_sentences = open(\"exercise/rt-polaritydata/rt-polarity.pos\").readlines()\n",
    "negative_sentences = open(\"exercise/rt-polaritydata/rt-polarity.neg\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tag(tokens):\n",
    "    doc = nlp(tokens)\n",
    "    return [t.pos_ for t in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET', 'NOUN', 'VERB', 'VERB', 'PART', 'VERB', 'PUNCT']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag(\"the rock is destined to be ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for speed reasons it might be more fruitful to actually store the POS tagged text, instead of tagging it for each experiment time from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding different transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "import spacy\n",
    "\n",
    "class PosFeatures(TransformerMixin): \n",
    "    \"\"\" using POS tags from Spacy \"\"\"\n",
    "    def __init__(self):\n",
    "        nlp = spacy.load('en')\n",
    "        \n",
    "    def _tag(tokens):\n",
    "        doc = nlp(tokens)\n",
    "        return [t.pos_ for t in doc]\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return [_tag(word) for word in X]\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        data={}\n",
    "        data['raw'] = X\n",
    "        data['pos'] = [\" \".join(tag(str(sentence))) for sentence in X]\n",
    "        print(len(X))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data..\n",
      "10662\n",
      "split data..\n",
      "tag data..\n",
      "vectorize data..\n",
      "train model..\n",
      "7996\n",
      "predict..\n",
      "2666\n",
      "Accuracy: 0.766316579145\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(113)\n",
    "\n",
    "def load_sentiment_sentences_and_labels():\n",
    "    \"\"\"\n",
    "    loads the movie review data\n",
    "    \"\"\"\n",
    "    positive_sentences = open(\"exercise/rt-polaritydata/rt-polarity.pos\").readlines()\n",
    "    negative_sentences = open(\"exercise/rt-polaritydata/rt-polarity.neg\").readlines()\n",
    "\n",
    "    positive_labels = [1 for sentence in positive_sentences]\n",
    "    negative_labels = [0 for sentence in negative_sentences]\n",
    "\n",
    "    sentences = np.concatenate([positive_sentences,negative_sentences], axis=0)\n",
    "    labels = np.concatenate([positive_labels,negative_labels],axis=0)\n",
    "\n",
    "    ## make sure we have a label for every data instance\n",
    "    assert(len(sentences)==len(labels))\n",
    "    data = list(zip(sentences,labels))\n",
    "    random.shuffle(data)\n",
    "      ## return the data (instances + labels)\n",
    "    return data\n",
    "\n",
    "## read input data\n",
    "print(\"load data..\")\n",
    "data = load_sentiment_sentences_and_labels()\n",
    "print(len(data))\n",
    "\n",
    "print(\"split data..\")\n",
    "split_point = int(0.75*len(data))\n",
    "\n",
    "print(\"tag data..\")\n",
    "sentences = [sentence for sentence, label in data]\n",
    "#sentences_tagged = [tag(str(sentence)) for sentence, _ in data]\n",
    "labels = [label for sentence, label in data]\n",
    "X_train, X_test = sentences[:split_point], sentences[split_point:]\n",
    "#X_train_pos, X_test_pos = sentences_tagged[:split_point], sentences_tagged[split_point:]\n",
    "\n",
    "y_train, y_test = labels[:split_point], labels[split_point:]\n",
    "\n",
    "assert(len(X_train)==len(y_train))\n",
    "assert(len(X_test)==len(y_test))\n",
    "\n",
    "print(\"vectorize data..\")\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('data',DataHandler()),\n",
    "        ('features', FeatureUnion([\n",
    "            ('bow', Pipeline([\n",
    "                ('selector', ItemSelector(key='raw')),\n",
    "                ('words', CountVectorizer()),\n",
    "                            ])),\n",
    "            ('pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='pos')),\n",
    "                ('words', CountVectorizer())\n",
    "                            ]))\n",
    "        ])),\n",
    "        ('classifier', LogisticRegression())])\n",
    "\n",
    "\n",
    "print(\"train model..\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"predict..\")\n",
    "y_predicted = pipeline.predict(X_test)\n",
    "\n",
    "###\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FeatureUnion` here gets pretty involved, but lets you nicely join heterogeneous feature spaces (although adding POS didn't help in our example). \n",
    "\n",
    "However, `FeatureUnion` does not support access to the feature names, thus we cannot run the `show_most_informative_features` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Transformer bow (type Pipeline) does not provide get_feature_names.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-8a97cddb0ec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mshow_most_informative_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classifier'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-151-8a97cddb0ec1>\u001b[0m in \u001b[0;36mshow_most_informative_features\u001b[0;34m(vectorizer, clf, n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_most_informative_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mcoefs_with_fns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoefs_with_fns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefs_with_fns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py35/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 raise AttributeError(\"Transformer %s (type %s) does not \"\n\u001b[1;32m    687\u001b[0m                                      \u001b[0;34m\"provide get_feature_names.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m                                      % (str(name), type(trans).__name__))\n\u001b[0m\u001b[1;32m    689\u001b[0m             feature_names.extend([name + \"__\" + f for f in\n\u001b[1;32m    690\u001b[0m                                   trans.get_feature_names()])\n",
      "\u001b[0;31mAttributeError\u001b[0m: Transformer bow (type Pipeline) does not provide get_feature_names."
     ]
    }
   ],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=10):\n",
    "    feature_names = vectorizer.get_feature_names() \n",
    "    for i in range(0,len(clf.coef_)):\n",
    "        coefs_with_fns = sorted(zip(clf.coef_[i], feature_names))\n",
    "        top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "        print(\"i\",i)\n",
    "        for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "            print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "\n",
    "\n",
    "show_most_informative_features(pipeline.named_steps['features'], pipeline.named_steps['classifier'], n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Using the `DictVectorizer`\n",
    "\n",
    "You can use the `DictVectorizer` directly, instead of using `FeatureUnion`. In that case, you create for every instance a dictionary with its features, and then give the list of dictionaries to your DictVectorizer. This has the advantage that you can later inspect the features.\n",
    "\n",
    "Lets write our own Featurizer. The advantage is that you have full control of what happens with the data, and you can later inspect your features.\n",
    "\n",
    "### Writing your own Featurizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict \n",
    "   \n",
    "class Featurizer(TransformerMixin):\n",
    "    \"\"\"Our own featurizer: extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    PREFIX_WORD_NGRAM=\"W:\"\n",
    "    PREFIX_CHAR_NGRAM=\"C:\"\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        here we could add more features!\n",
    "        \"\"\"\n",
    "        out= [self._word_ngrams(text,ngram=self.word_ngrams)\n",
    "                for text in X]\n",
    "        return out\n",
    "\n",
    "    def __init__(self,word_ngrams=\"1\",binary=True,lowercase=False,remove_stopwords=False):\n",
    "        \"\"\"\n",
    "        binary: whether to use 1/0 values or counts\n",
    "        lowercase: convert text to lowercase\n",
    "        remove_stopwords: True/False\n",
    "        \"\"\"\n",
    "        self.DELIM=\" \"\n",
    "        self.data = [] # will hold data (list of dictionaries, one for every instance)\n",
    "        self.lowercase=lowercase\n",
    "        self.binary=binary\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.stopwords = stopwords.words('english')\n",
    "        self.word_ngrams=word_ngrams\n",
    "\n",
    "        \n",
    "    def _word_ngrams(self,text,ngram=\"1-2-3\"):\n",
    "        \"\"\"\n",
    "        extracts word n-grams\n",
    "\n",
    "        >>> f=Featurizer()\n",
    "        >>> d = f._word_ngrams(\"this is a test\",ngram=\"1-3\")\n",
    "        >>> len(d)\n",
    "        6\n",
    "        \"\"\"\n",
    "        d={} #dictionary that holds features for current instance\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        words=text.split(self.DELIM)\n",
    "        if self.remove_stopwords:\n",
    "            words = [w for w in words if w not in self.stopwords]\n",
    "\n",
    "        for n in ngram.split(\"-\"):\n",
    "            for gram in nltk.ngrams(words, int(n)):\n",
    "                gram = self.PREFIX_WORD_NGRAM + \"_\".join(gram)\n",
    "                if self.binary:\n",
    "                     d[gram] = 1 #binary\n",
    "                else:\n",
    "                    d[gram] += 1\n",
    "        return d\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "split data..\n",
      "vectorize data..\n",
      "train model..\n",
      "predict..\n",
      "Accuracy: 0.765191297824\n"
     ]
    }
   ],
   "source": [
    "random.seed(113)\n",
    "X_train, y_train, X_test, y_test = load_sentiment_sentences_and_labels()\n",
    "\n",
    "print(\"vectorize data..\", file=sys.stderr)\n",
    "featurizer = Featurizer(word_ngrams=\"1-2\")\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "# first extract the features (as dictionaries)\n",
    "X_train_dict = featurizer.fit_transform(X_train)\n",
    "X_test_dict = featurizer.transform(X_test)\n",
    "\n",
    "# then convert them to the internal representation (maps each feature to an id)\n",
    "X_train = vectorizer.fit_transform(X_train_dict)\n",
    "X_test = vectorizer.transform(X_test_dict)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "print(\"train model..\", file=sys.stderr)\n",
    "classifier.fit(X_train, y_train)\n",
    "##\n",
    "print(\"predict..\", file=sys.stderr)\n",
    "y_predicted = classifier.predict(X_test)\n",
    "###\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "\t-1.6217\tW:too          \t\t1.2806\tW:works        \n",
      "\t-1.5755\tW:bad          \t\t1.2107\tW:rare         \n",
      "\t-1.5396\tW:dull         \t\t1.1733\tW:entertaining \n",
      "\t-1.2782\tW:boring       \t\t1.0934\tW:beautiful    \n",
      "\t-1.2193\tW:worst        \t\t1.0891\tW:engrossing   \n",
      "\t-1.1327\tW:tedious      \t\t1.0795\tW:cinema       \n",
      "\t-1.1248\tW:lacks        \t\t1.0736\tW:the_best     \n",
      "\t-1.0898\tW:mess         \t\t1.0552\tW:funny        \n",
      "\t-1.0781\tW:plodding     \t\t1.0408\tW:always       \n",
      "\t-1.0755\tW:stupid       \t\t1.0213\tW:wonderful    \n",
      "\t-1.0569\tW:no           \t\t1.0171\tW:fun          \n",
      "\t-1.0526\tW:flat         \t\t1.0157\tW:culture      \n",
      "\t-1.0454\tW:only         \t\t0.9988\tW:brilliant    \n",
      "\t-1.0204\tW:video        \t\t0.9711\tW:solid        \n",
      "\t-1.0043\tW:the_worst    \t\t0.9650\tW:beautifully  \n",
      "\t-1.0002\tW:mediocre     \t\t0.9564\tW:powerful     \n",
      "\t-0.9967\tW:neither      \t\t0.9540\tW:still        \n",
      "\t-0.9903\tW:barely       \t\t0.9437\tW:delivers     \n",
      "\t-0.9826\tW:pretentious  \t\t0.9264\tW:refreshing   \n",
      "\t-0.9805\tW:tv           \t\t0.9073\tW:charming     \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(vectorizer, classifier, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "\n",
    "1. Extend the `Featurizer` to include character n-grams.\n",
    "2. Extend the `Featurizer` with POS features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I know which class number corresponds to which target?\n",
    "\n",
    "A very handy class that takes care for the mapping between class number and name is the `LabelEncoder`.\n",
    "\n",
    "Here is an example from the documentation (http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\", \"paris\"]) # give it your y_train labels!\n",
    "list(le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le.transform([\"tokyo\", \"tokyo\", \"paris\"]) \n",
    "example_class_nums = [2, 2, 1]\n",
    "list(le.inverse_transform(example_class_nums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. A detailed example on the 20-newsgroup dataset, from which parts of this tutorial are taken: [sklearn feature union](http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#sphx-glr-auto-examples-hetero-feature-union-py)\n",
    "2. http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "3. https://michelleful.github.io/code-blog/2015/06/20/pipelines/\n",
    "4. My own code for the recent IJCNLP 2017 shared task 4: https://github.com/bplank/ijcnlp2017-customer-feedback/blob/master/src/classifier.py (described in [this paper](http://www.let.rug.nl/~bplank/papers/ijcnlp2017_shared_task4_bplank.pdf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
